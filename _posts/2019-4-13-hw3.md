---
layout:     post                    # 使用的布局（不需要改）
title:      Some details about the K-nearest neighbors( KNN )               # 标题 
subtitle:   Microeconometrics #副标题
date:       2019-03-29              # 时间
author:     ELVIS                      # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - Microeconometrics
---
<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
# Some details about the K-nearest neighbors( KNN )       

KNN was proposed by Hart and Cover, whose original purpose is accomplishing the text classsification work. [1] The 
basic principle behind this algorithm is that, for a given unclassified sample X, we can calculate the likelihood between X and the training set then find K samples that are most similar with the sample X, therefore we are now able to estimate the value of X based on the most common labels among the chosen K samples.       

This algorithm is constructed on the limited neighbor samples rather than the region the sample belongs to. Due to this KNN is much more valuable for sample sets that exists many crossover or overlaps within which.      

## the algorithm realization process  

There exists several kinds of implementation method, the key difference between them lies on the choice of "distance" between neighbor samples. I'll explain that in the following steps. Now we are going to just focus on the traditonal way.    

* At first we need to build our training set. Name it as $$\omega $$, $$\omega = \left \{ \left ( x_i,c_i \right ) \right |i=1,2\cdots n\}$$,where**x**is a vector in dimension *l*,$$x_i^h$$shows the value h in sample i,$$c_i$$means the category that sample i belongs to.     

* Then we usually choose an initial value of K and make some adjustments in accordance with our training result. I need to mention that we are used to choosing K as an odd value such as 3, 5, 7. Here for simplicity I'll take 3.   

* Name the test set as$$\varphi $$,$$\varphi = \left \{ y_j|j=1,2,\cdots ,m \right \}$$,where$$y_j$$is the value h in test sample j. Then it comes to the vital step: calculating the "distance" between neighbours. For example, the distance between training sample i and test sample j is   

$$d(x_i, y_j) = \sqrt{\sum_{h=1}^l(x_i^h-y_j^h)^2}$$    

* Find the nearest K samples in our training set to test sample $$y_i$$, name them as $$x_1$$, $$x_2$$, $$\cdots$$,$$x_k$$. Now we need to classify them. Construct a discrete function *f*.     

$$\hat{f(y_i)}=arg max_{c\in C}\sum_{h=1}^K \delta(c,f(x_i))$$    

   

* if $$m=n$$ then we have $$\delta(a,b)=1$$, otherwise $$\delta(a,b)=0$$.  Finally， $$\hat{f(y_i)}$$ is the category of $$y_i$$.


