---
layout:     post                    # 使用的布局（不需要改）
title:      Some details about the K-nearest neighbors( KNN )               # 标题 
subtitle:   Microeconometrics #副标题
date:       2019-03-29              # 时间
author:     ELVIS                      # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - Microeconometrics
---

# Some details about the K-nearest neighbors( KNN )       

KNN was proposed by Hart and Cover, whose original purpose is accomplishing the text classsification work. [1] The 
basic principle behind this algorithm is that, for a given unclassified sample X, we can calculate the likelihood between X and the training set then find K samples that are most similar with the sample X, therefore we are now able to estimate the value of X based on the most common labels among the chosen K samples.       

This algorithm is constructed on the limited neighbor samples rather than the region the sample belongs to. Due to this KNN is much more valuable for sample sets that exists many crossover or overlaps within which.      

## the algorithm realization process  

There exists several kinds of implementation method, the key difference between them lies on the choice of "distance" between neighbor samples. I'll explain that in the following steps. Now we are going to just focus on the traditonal way.      

- At first we need to build our training set, which can be shown as \Omega, \Omega = {(**x**,
