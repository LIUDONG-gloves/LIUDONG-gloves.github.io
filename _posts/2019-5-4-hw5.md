---
layout:     post                    
title:      RandomForest and DecisionTree practice        # 标题 
subtitle:   Microeconometrics #副标题
date:       2019-04-13              # 时间
author:     ELVIS                      # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - Microeconometrics
---
<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

# RandomForest and DecisionTree practice    

Random Forest( RF ) is one kind of expansion of Bagging, it takes Decision Tree as its basic classifier. The key difference between RF and typical decision tree happens during the choice of attribute separation, where the former one is used to randomly picking up one subset containing *k* attributes first then decide the optimal attribute, while the later one just make the choice of the best attribute within the current attribute sets directly.     

Here I will present detailedly my python code of both two algorithm. The dataset is a classification data about glass made by UCI. The url has been put below so you can go check it.     

```
import pandas as pd
def ReadAndSaveDataByPandas(target_url=None, save=False):
    wine = pd.read_csv(target_url, header=0, sep=";")
    if save == True:
        wine.to_csv(r"D:\Documents\ml_data\carbon_nanotubes.csv",
                    sep=' ', index=False) 
# do create the same directory or the code will exit
target_url = "https://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data"  
ReadAndSaveDataByPandas(target_url, True)
```   

The code above is for requiring the glass dataset.   

## Decision Tree  

### INformation entropy    

Information entropy is a widely used measure of the set purity of sample. If there are N samples belonging to d categories in set D, let $p_k=N_k/N$ stands for the probability of selecting a *k* sample randomly, them we have:    

$Ent\left ( D \right )=-\sum_{k=1}^{d}p_k\log_{2}^{p_k}$    

Assume there are *V* underlying values of one discrete attribute a, if we divide sample *D* in accordance with property a, then V branches will be created, and we denote the sample amount in branch *v* as $N_v$, then we are able to calculate the information entropy of all branches, we wed it together with there weights in total sample, get the definition of information entropy below:   

$Gain\left ( D,a \right )=Ent\left ( D \right )-\sum_{v=1}^{V}\frac{N_v}{N}Ent\left ( D_v \right )$    

Usually higher information entropy represents better purity of our sample based on attribute a.    

### Gain ratio   
According to materials I found, the largest $Gain\left ( D,a \right )$ does not always match the best attribution as it lacks of the generalization ability. Now I will introduce "gain ratio" to you guys.    

$GainRatio\left ( D,a \right )=\frac{Gain\left ( D,a \right )}{IV\left ( a \right )}$   
$IV\left ( a \right )=-\sum_{v=1}^{V}\frac{N_v}{N}\log_{2}^{\frac{N_v}{N}}$    

We choose property with the highest gain ratio as our best attribute. $IV\left ( a \right )$ is the nature property of attribut a. As far as I know, this gain ratio may perfer attribute which target sample amount is releatively lower, thus I need to calculate gain ratios for all attributes, drop half which gain ratios are below the average level.    

### Gini index   

other method for constructing our decision tree is Gini index. If we select two indiciduals from a large sample, the probability that they belong to different categories is called Gini.   

$Gini\left ( D \right )=1-\sum_{v=1}^{d}p_v^2$   

It's direct that smaller Gini means purer dataset. It can be used to select attributes after acquire Gini Index:   
$GiniIndex\left ( D,a \right )=\sum_{v=1}^{V}\frac{N_v}{N}Gini\left ( D_v \right )$  

We choose the attribute with the lowest Gini Index as our target, and we get what is called as CART decision tree.   


```
import pandas as pd
def ReadAndSaveDataByPandas(target_url=None, save=False):
    wine = pd.read_csv(target_url, header=0, sep=";")
    if save == True:
        wine.to_csv(r"D:\Documents\ml_data\carbon_nanotubes.csv",
                    sep=' ', index=False)
target_url = "https://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data"  # 一个玻璃的多分类的数据集
ReadAndSaveDataByPandas(target_url, True)


import numpy as np
import math
from collections import Counter


class decisionnode:
    def __init__(self, d=None, thre=None, results=None, NH=None, lb=None, rb=None, max_label=None):
        self.d = d   # d表示维度
        self.thre = thre  # thre表示二分时的比较值，将样本集分为2类
        self.results = results  # 最后的叶节点代表的类别
        self.NH = NH  # 存储各节点的样本量与经验熵的乘积，便于剪枝时使用
        self.lb = lb  # desision node,对应于样本在d维的数据小于thre时，树上相对于当前节点的子树上的节点
        self.rb = rb  # desision node,对应于样本在d维的数据大于thre时，树上相对于当前节点的子树上的节点
        self.max_label = max_label  # 记录当前节点包含的label中同类最多的label


def entropy(y):
    '''
    计算信息熵，y为labels
    '''

    if y.size > 1:

        category = list(set(y))

    else:

        category = [y.item()]
        y = [y.item()]

    ent = 0

    for label in category:
        p = len([label_ for label_ in y if label_ == label]) / len(y)
        ent += -p * math.log(p, 2)

    return ent


def Gini(y):
    '''
    计算基尼指数，y为labels
    '''
    category = list(set(y))
    gini = 1

    for label in category:
        p = len([label_ for label_ in y if label_ == label]) / len(y)
        gini += -p * p

    return gini


def GainEnt_max(X, y, d):
    '''
    计算选择属性attr的最大信息增益，X为样本集,y为label，d为一个维度，type为int
    '''
    ent_X = entropy(y)
    X_attr = X[:, d]
    X_attr = list(set(X_attr))
    X_attr = sorted(X_attr)
    Gain = 0
    thre = 0

    for i in range(len(X_attr) - 1):
        thre_temp = (X_attr[i] + X_attr[i + 1]) / 2
        y_small_index = [i_arg for i_arg in range(
            len(X[:, d])) if X[i_arg, d] <= thre_temp]
        y_big_index = [i_arg for i_arg in range(
            len(X[:, d])) if X[i_arg, d] > thre_temp]
        y_small = y[y_small_index]
        y_big = y[y_big_index]

        Gain_temp = ent_X - (len(y_small) / len(y)) * \
            entropy(y_small) - (len(y_big) / len(y)) * entropy(y_big)
        '''
        intrinsic_value = -(len(y_small) / len(y)) * math.log(len(y_small) /
                                                              len(y), 2) - (len(y_big) / len(y)) * math.log(len(y_big) / len(y), 2)
        Gain_temp = Gain_temp / intrinsic_value
        '''
        # print(Gain_temp)
        if Gain < Gain_temp:
            Gain = Gain_temp
            thre = thre_temp
    return Gain, thre


def Gini_index_min(X, y, d):
    '''
    计算选择属性attr的最小基尼指数，X为样本集,y为label，d为一个维度，type为int
    '''

    X = X.reshape(-1, len(X.T))
    X_attr = X[:, d]
    X_attr = list(set(X_attr))
    X_attr = sorted(X_attr)
    Gini_index = 1
    thre = 0

    for i in range(len(X_attr) - 1):
        thre_temp = (X_attr[i] + X_attr[i + 1]) / 2
        y_small_index = [i_arg for i_arg in range(
            len(X[:, d])) if X[i_arg, d] <= thre_temp]

        y_big_index = [i_arg for i_arg in range(
            len(X[:, d])) if X[i_arg, d] > thre_temp]
        y_small = y[y_small_index]
        y_big = y[y_big_index]

        Gini_index_temp = (len(y_small) / len(y)) * \
            Gini(y_small) + (len(y_big) / len(y)) * Gini(y_big)
        if Gini_index > Gini_index_temp:
            Gini_index = Gini_index_temp
            thre = thre_temp
    return Gini_index, thre


def attribute_based_on_GainEnt(X, y):
    '''
    基于信息增益选择最优属性，X为样本集，y为label
    '''
    D = np.arange(len(X[0]))
    Gain_max = 0
    thre_ = 0
    d_ = 0
    for d in D:
        Gain, thre = GainEnt_max(X, y, d)
        if Gain_max < Gain:
            Gain_max = Gain
            thre_ = thre
            d_ = d  # 维度标号

    return Gain_max, thre_, d_


def attribute_based_on_Giniindex(X, y):
    '''
    基于信息增益选择最优属性，X为样本集，y为label
    '''
    D = np.arange(len(X.T))
    Gini_Index_Min = 1
    thre_ = 0
    d_ = 0
    for d in D:
        Gini_index, thre = Gini_index_min(X, y, d)
        if Gini_Index_Min > Gini_index:
            Gini_Index_Min = Gini_index
            thre_ = thre
            d_ = d  # 维度标号

    return Gini_Index_Min, thre_, d_


def devide_group(X, y, thre, d):
    '''
    按照维度d下阈值为thre分为两类并返回
    '''
    X_in_d = X[:, d]
    x_small_index = [i_arg for i_arg in range(
        len(X[:, d])) if X[i_arg, d] <= thre]
    x_big_index = [i_arg for i_arg in range(
        len(X[:, d])) if X[i_arg, d] > thre]

    X_small = X[x_small_index]
    y_small = y[x_small_index]
    X_big = X[x_big_index]
    y_big = y[x_big_index]
    return X_small, y_small, X_big, y_big


def NtHt(y):
    '''
    计算经验熵与样本数的乘积，用来剪枝，y为labels
    '''
    if len(y) == 0:
        return 0.1
    ent = entropy(y)
    print('ent={},y_len={},all={}'.format(ent, len(y), ent * len(y)))
    return ent * len(y)


def maxlabel(y):
    print(y)
    label_ = Counter(y).most_common(1)
    return label_[0][0]


def ChooseSubsetForRF(X, y, d):
    '''
    d为属性的个数
    '''
    k = int(np.log2(d))
    index = np.random.choice(d, k, replace=False)
    X_sub = X[:, index]
    return X_sub, index


def buildtree(X, y, method='Gini'):
    '''
    递归的方式构建决策树
    '''
    if y.size > 1:
        X_sub, d_index = ChooseSubsetForRF(X, y, d=X.shape[1])
        if method == 'Gini':
            Gain_max, thre, d = attribute_based_on_Giniindex(X_sub, y)
        elif method == 'GainEnt':
            Gain_max, thre, d = attribute_based_on_GainEnt(X_sub, y)
        if (Gain_max > 0 and method == 'GainEnt') or (Gain_max >= 0 and len(list(set(y))) > 1 and method == 'Gini'):
            X_small, y_small, X_big, y_big = devide_group(
                X, y, thre, d_index[d])
            left_branch = buildtree(X_small, y_small, method=method)
            right_branch = buildtree(X_big, y_big, method=method)
            nh = NtHt(y)
            max_label = maxlabel(y)
            return decisionnode(d=d_index[d], thre=thre, NH=nh, lb=left_branch, rb=right_branch, max_label=max_label)
        else:
            nh = NtHt(y)
            max_label = maxlabel(y)
            return decisionnode(results=y[0], NH=nh, max_label=max_label)
    elif y.size == 1:
        nh = NtHt(y)
        max_label = maxlabel(y)
        return decisionnode(results=y.item(), NH=nh, max_label=max_label)
def classify(observation, tree):
    if tree.results != None:
        return tree.results
    else:
        v = observation[tree.d]
        branch = None

        if v > tree.thre:
            branch = tree.rb
        else:
            branch = tree.lb

        return classify(observation, branch)


import pandas as pd
import numpy as np
import DecisionTree
from collections import Counter
from sklearn.ensemble import RandomForestClassifier


class RF():
    def __init__(self, num=5):
        self.num = num  # 基分类器数量

    def random_Xy(self, X, y, per=80):
        '''
        per表示要取出的X，y中的数量占原来的比重，默认为80%
        '''
        index = np.random.choice(y.shape[0], int(
            y.shape[0] * per / 100), replace=False)
        return X[index], y[index]

    def fit(self, X, y):
        self.tree = []
        for i in range(self.num):
            X_r, y_r = self.random_Xy(X, y)
            self.tree.append(buildtree(X_r, y_r, method='Gini'))

    def predict(self, x):
        results = []
        for i in range(self.num):
            results.append(classify(x, self.tree[i]))

        return Counter(results).most_common(4)[0][0]


if __name__ == '__main__':

    dir = 'D:\\Documents\\ml_data\\'
    name = 'carbon_nanotubes.csv'

    obj = pd.read_csv(dir + name, header=None)
    data = np.array(obj[:][:])
    label = data[:, -1]

    label = label.astype(int)
    data = data[:, 1:data.shape[1] - 1]

    test_index = np.random.choice(label.shape[0], 50, replace=False)
    test_label = label[test_index]
    test_data = data[test_index]

    train_index = np.linspace(
        0, data.shape[0], num=data.shape[0], endpoint=False, dtype=int)
    train_index = np.delete(train_index, test_index, axis=0)

    train_label = label[train_index]
    train_data = data[train_index]

    rf = RF(num=50)
    rf.fit(train_data, train_label)

    true_count = 0
    for i in range(len(test_label)):
        predict = rf.predict(test_data[i])
        if predict == test_label[i]:
            true_count += 1
    print(true_count / test_label.shape[0])

    clf = RandomForestClassifier(n_estimators=50)
    clf = clf.fit(train_data, train_label)
    pl = clf.predict(test_data)
    diff = pl - test_label

    count1 = 0
    for i in diff:
        if i == 0:
            count1 += 1
    print(count1 / test_label.shape[0])
```

