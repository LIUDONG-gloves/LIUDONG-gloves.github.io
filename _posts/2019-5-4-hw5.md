---
layout:     post                    
title:      RandomForest and DecisionTree practice        # 标题 
subtitle:   Microeconometrics #副标题
date:       2019-04-13              # 时间
author:     ELVIS                      # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - Microeconometrics
---
<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

# RandomForest and DecisionTree practice    

Random Forest( RF ) is one kind of expansion of Bagging, it takes Decision Tree as its basic classifier. The key difference between RF and typical decision tree happens during the choice of attribute separation, where the former one is used to randomly picking up one subset containing *k* attributes first then decide the optimal attribute, while the later one just make the choice of the best attribute within the current attribute sets directly.     

Here I will present detailedly my python code of both two algorithm. The dataset is a classification data about glass made by UCI. The url has been put below so you can go check it.     

```
import pandas as pd
def ReadAndSaveDataByPandas(target_url=None, save=False):
    wine = pd.read_csv(target_url, header=0, sep=";")
    if save == True:
        wine.to_csv(r"D:\Documents\ml_data\carbon_nanotubes.csv",
                    sep=' ', index=False) 
# do create the same directory or the code will exit
target_url = "https://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data"  
ReadAndSaveDataByPandas(target_url, True)
```   

The code above is for requiring the glass dataset.   

## Decision Tree  



