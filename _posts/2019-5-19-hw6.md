---
layout:     post                    
title:      666        # 标题 
subtitle:   Microeconometrics #副标题
date:       2019-05-19              # 时间
author:     ELVIS                      # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - Microeconometrics
---
<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>



# Back Propagation   

The basic idea of backpropagation is to adjust the network parameters by calculating the error between the output layer and the expected value, so that the error becomes smaller.
In fact, the principle of backpropagation is based on only four equations, which will be further introduced later.     

## Prerequisite knowledge - linear algebra  

### Gradient matrix   

Assuming that function $$f:R^{m\times n}\rightarrow R_{i}$$ can map input matrix( shape: $$m\times n$$) to a real number, the gradient of function $$f$$ is defined as:  

$$\left ( \bigtriangledown_Af\left ( A \right ) \right )_{ij}=\begin{bmatrix}
\frac{\partial f\left ( A \right )}{\partial A_{11}} & \frac{\partial f\left ( A \right )}{\partial A_{12}} & \cdots & \frac{\partial f\left ( A \right )}{\partial A_{1n}} & \\ 
\frac{\partial f\left ( A \right )}{\partial A_{21}} & \frac{\partial f\left ( A \right )}{\partial A_{22}} & \cdots  & \frac{\partial f\left ( A \right )}{\partial A_{2n}} & \\ 
\vdots  & \vdots  & \ddots  & \vdots  & \\ 
\frac{\partial f\left ( A \right )}{\partial A_{m1}} & \frac{\partial f\left ( A \right )}{\partial A_{m2}} & \cdots &  \frac{\partial f\left ( A \right )}{\partial A_{mn}} & 
\end{bmatrix}$$   

In other words:  

$$\left ( \bigtriangledown_Af\left ( A \right ) \right )_{ij}=\frac{\partial f\left ( A \right )}{\partial A_{ij}}$$   

Similarly, a function $$f:R^{n\times 1}\rightarrow R_{i}$$ that is a vector( vector generally refers to a colunmn vector, in this case which refers to a column vector by default without special declaration) 
has:   

$$\bigtriangledown_Af\left ( A \right ) = \begin{bmatrix}
\frac{\partial f\left ( x \right )}{\partial x_1}\\ 
\frac{f\left ( x \right )}{\partial x_2}\\ 
\vdots \\
\frac{f\left ( x \right )}{\partial x_n}\\  
\end{bmatrix}
$$   

The premise of the gradient solution involved here is that function *f* returns a real number. If the function returns a matrix or a vector, then there is no way to find a gradient. For example, to function$$f\left ( A \right )=\sum_{i=0}^{m}\sum_{j=0}^{n}A_{ij}^{2}$$ we can solve the gradient matrix by returning a real number, if $$f\left ( x \right )=Ax\left ( A\epsilon R^{m\times n},x\epsilon R^{n\times 1} \right )$$. Since the function returns a vector with $$m$$ rows and $$1$$ column, the gradient matrix cannot be obtained for $$f$$.   

By definition, it is easy to get the following properties:  

$$\bigtriangledown _{x}\left ( f\left ( x \right ) + g\left ( x \right )\right) =\bigtriangledown_{x}f\left(x\right)+\bigtriangledown_{x}g\left ( x \right )$$  
$$\bigtriangledown\left ( tf\left ( x \right ) \right )=t \bigtriangledown f\left ( x \right ), t\epsilon R$$   

WIth above knowledge, we can simulate one example:  

Defining function $$f:R^{m}\rightarrow R,f\left ( z \right )=z^{T}z$$, we can acquire$$ \bigtriangledown_{z}f\left ( z \right )=2z$$.   

### Hessian Matrix  

Define a function $$f: R^{n}\rightarrow R$$ whose input is an n-dimensional vector with a real number output. Then the Hessian Matrix is defined as the square matrix of the second-order partial derivatives of the multivariate funcion $$f$$:  

$$\bigtriangledown _{x}^{2}f\left ( x \right )=\begin{bmatrix}
\frac{\partial^2 f\left ( x \right )}{\partial x_1^2} & \frac{\partial^2 f\left ( x \right )}{\partial x_1\partial x_2} & \cdots & \frac{\partial^2 f\left ( x \right )}{\partial x_1\partial x_n} & \\ 
\frac{\partial^2 f\left ( x \right )}{\partial x_2\partial x_1} & \frac{\partial^2 f\left ( x \right )}{\partial x_2^2} & \cdots  & \frac{\partial^2 f\left ( x \right )}{\partial x_2\partial x_n} & \\ 
\vdots  & \vdots  & \ddots  & \vdots  & \\ 
\frac{\partial^2 f\left ( x \right )}{\partial x_n\partial x_1} & \frac{\partial^2 f\left ( x \right )}{\partial x_n\partial x_2} & \cdots &  \frac{\partial^2 f\left ( x \right )}{\partial x_n^2} & 
\end{bmatrix}$$  

It can be withdrown from the equation above that the Hessian Matrix is always a symmetric matrix.   

### Short Summary  





